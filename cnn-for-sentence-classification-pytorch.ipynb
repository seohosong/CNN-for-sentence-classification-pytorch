{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import unicodedata\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from time import strftime, gmtime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = glob.glob('txt_sentoken/neg/*.txt')\n",
    "pos_list = glob.glob('txt_sentoken/pos/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_txt in pos_list:\n",
    "    with open(pos_txt, 'r', encoding='utf-8') as f:\n",
    "        pos_sentences += f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neg_txt in neg_list:\n",
    "    with open(neg_txt, 'r', encoding='utf-8') as f:\n",
    "        neg_sentences += f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    output = []\n",
    "    for s in sentences:\n",
    "        s = unicodeToAscii(s)\n",
    "        #s = normalizeString(s)\n",
    "        s = clean_txt(s)\n",
    "        output.append(s)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = preprocess(pos_sentences)\n",
    "neg_sentences = preprocess(neg_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(sentences, positive=True):\n",
    "    pairs = []\n",
    "    for s in sentences:\n",
    "        if positive:\n",
    "            pairs.append((s, 1))\n",
    "        else:\n",
    "            pairs.append((s, 0))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pairs = create_pairs(pos_sentences, True)\n",
    "neg_pairs = create_pairs(neg_sentences, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_pairs = pos_pairs + neg_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(whole_pairs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2tensor(sentence, dictionary, eval=False, fixed_len=70):\n",
    "    sen = sentence.split(' ')\n",
    "    output = []\n",
    "    if len(sen) < fixed_len:\n",
    "        while len(sen) < fixed_len:\n",
    "            sen.append('<PAD>')\n",
    "    else:\n",
    "        sen = sen[:fixed_len]\n",
    "\n",
    "    for word in sen:\n",
    "        if eval and word not in dictionary.word2ix:\n",
    "            output.append(dictionary.word2ix['<UNK>'])\n",
    "        else:\n",
    "            output.append(dictionary.word2ix[word])\n",
    "    output = torch.LongTensor(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, pairs, dictionary, fixed_length=70):\n",
    "        #self.pos_sen = pos_pair\n",
    "        #self.neg_sen = neg_pair\n",
    "        self.pairs = pairs\n",
    "        self.dictionary = dictionary\n",
    "        #self.dataset = pos_pair + neg_pair\n",
    "        #self.pos_len = len(pos_pair)\n",
    "        #self.neg_len = len(neg_pair)\n",
    "        self.fixed_len = fixed_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        out = sen2tensor(self.pairs[ix][0], self.dictionary, fixed_len=self.fixed_len)\n",
    "        if self.pairs[ix][1] == 1:\n",
    "            label = torch.tensor(1)\n",
    "        else:\n",
    "            label = torch.tensor(0)\n",
    "        \n",
    "        return out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.word2ix = {'<PAD>':0, '<UNK>':1}\n",
    "        self.ix2word = {0 : '<PAD>', 1 : '<UNK>'}\n",
    "        self.n_words = 2\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence[0].split(' ')\n",
    "            for word in sentence:\n",
    "                if word not in self.word2ix:\n",
    "                    self.word2ix[word] = self.n_words\n",
    "                    self.ix2word[self.n_words] = word\n",
    "                    self.n_words += 1\n",
    "                else:\n",
    "                     continue\n",
    "        print(self.n_words, 'counted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40696 counted\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dict(whole_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset(train_split, dictionary)\n",
    "testset = Dataset(test_split, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pretrained_embed(path, dictionary):\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    \n",
    "    vector = []\n",
    "    for word in dictionary.word2ix:\n",
    "        if word in word2vec.vocab:\n",
    "            vector.append(word2vec[word])\n",
    "        else:\n",
    "            vector.append(np.random.uniform(-0.01, 0.01, 300))\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "pretrained = read_pretrained_embed('GoogleNews-vectors-negative300.bin', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, dictionary, embed_dim, kernel_sizes, vector=None, pretrained=True, num_features=100, dropout=0.5, output_size=2):\n",
    "        super(Convnet, self).__init__()\n",
    "        self.embedding = nn.Embedding(dictionary.n_words, embed_dim)\n",
    "        if pretrained:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(vector))\n",
    "        \n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_features = num_features\n",
    "        self.dropout = dropout\n",
    "                \n",
    "        for size in kernel_sizes:\n",
    "            setattr(self, 'conv_'+str(size), nn.Conv1d(1, num_features, embed_dim * size, stride=embed_dim))\n",
    "        \n",
    "        self.linear1 = nn.Linear(len(kernel_sizes) * num_features, 128)\n",
    "        self.linear2 = nn.Linear(128, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        x = self.embedding(x).view(batch_size, 1, -1)\n",
    "                   \n",
    "        conv_outputs = [F.max_pool1d(F.relu(getattr(self, 'conv_'+str(filter_size))(x)), seq_len - filter_size +1).view(-1, self.num_features) \n",
    "                        for filter_size in self.kernel_sizes]\n",
    "        \n",
    "        out = torch.cat(conv_outputs, 1)\n",
    "        out = F.dropout(out, self.dropout)\n",
    "        out = F.dropout(F.relu(self.linear1(out)), self.dropout)\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Convnet(dictionary, 300, [3,4,5], pretrained, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Convnet(\n",
       "  (embedding): Embedding(40696, 300)\n",
       "  (conv_3): Conv1d(1, 100, kernel_size=(900,), stride=(300,))\n",
       "  (conv_4): Conv1d(1, 100, kernel_size=(1200,), stride=(300,))\n",
       "  (conv_5): Conv1d(1, 100, kernel_size=(1500,), stride=(300,))\n",
       "  (linear1): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(trainset, batch_size=50, shuffle=True)\n",
    "testloader = data.DataLoader(testset, batch_size=50, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    writer = SummaryWriter(log_dir='log/')# + strftime('%H:%M:%S', gmtime()))\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for epoch in tqdm.tnrange(10):\n",
    "        net.train()\n",
    "        \n",
    "        acc = 0.0\n",
    "        total = 0.0\n",
    "        training_loss = 0.0\n",
    "        max_test_acc = 0.0\n",
    "        \n",
    "        for i, (data, target) in enumerate(tqdm.tqdm_notebook(trainloader)):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)#.unsqueeze(1).float()\n",
    "            output = net(data)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            _, max = output.max(dim=1)\n",
    "            acc += (max == target).sum().item()\n",
    "            total += output.shape[0]\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=3.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 1000 == 0:\n",
    "                print('epoch %d | step %d | loss %0.4f | train accuracy %0.2f %%' %(epoch+1, i+1, training_loss/1000, 100*acc/total))\n",
    "                training_loss = 0.0\n",
    "        \n",
    "        test_loss, test_acc = test()\n",
    "        writer.add_scalar('loss/train', epoch_loss/total, epoch)\n",
    "        writer.add_scalar('acc/train', acc/total, epoch)\n",
    "        writer.add_scalar('loss/test', test_loss, epoch)\n",
    "        writer.add_scalar('acc/test', test_acc, epoch)\n",
    "        \n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            best_model = copy.deepcopy(net)\n",
    "            best_epoch = epoch+1\n",
    "    writer.close()\n",
    "    \n",
    "    torch.save(best_model, 'best_model_epoch_{}.pt'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, label) in enumerate(testloader):\n",
    "            data, label = data.to(device), label.to(device)#.unsqueeze(1)\n",
    "\n",
    "            out = net(data)\n",
    "            loss = criterion(out, label)\n",
    "            test_loss += loss.item()\n",
    "            topk, topi = out.max(dim=1)\n",
    "            correct += (label == topi).sum().item()\n",
    "            total += out.shape[0]\n",
    "    \n",
    "    print('Test accuracy : %0.2f %%' %(100 * correct / total))\n",
    "    \n",
    "    return test_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c534c5d37548b3877da60ee15d06e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770c9f9775e4498e9c13b14170c67951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | step 1000 | loss 0.5622 | train accuracy 70.87 %\n",
      "Test accuracy : 64.35 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b41c5c1c5249bea089210ede18f9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | step 1000 | loss 0.5305 | train accuracy 73.49 %\n",
      "Test accuracy : 65.35 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25dbf5ff00a4f3ca063a19016403e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 | step 1000 | loss 0.4958 | train accuracy 75.81 %\n",
      "Test accuracy : 64.92 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f630473cbab24d22938158ed20efe7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 | step 1000 | loss 0.4606 | train accuracy 77.97 %\n",
      "Test accuracy : 63.64 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d367a7a24b3f4031a12e6cce1ed722f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | step 1000 | loss 0.4278 | train accuracy 80.17 %\n",
      "Test accuracy : 64.76 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82ced7dad8b4e8ea0884e0395b7bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 | step 1000 | loss 0.3941 | train accuracy 81.81 %\n",
      "Test accuracy : 65.43 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6384812eae9449b08c204fde5fe370ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 | step 1000 | loss 0.3566 | train accuracy 83.94 %\n",
      "Test accuracy : 64.97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91fbaf401a54feabec4c41b5e311b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 | step 1000 | loss 0.3355 | train accuracy 85.19 %\n",
      "Test accuracy : 65.66 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e18cd3a5144effbee2c5da68d724a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 | step 1000 | loss 0.3067 | train accuracy 86.76 %\n",
      "Test accuracy : 64.75 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013ead4c3ab45eeb35ffed25ef41a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | step 1000 | loss 0.2857 | train accuracy 87.51 %\n",
      "Test accuracy : 64.12 %\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, dictionary):\n",
    "    \n",
    "    data = sen2tensor(sentence, dictionary, eval=True).unsqueeze(0).to(device)\n",
    "    output = net(data)\n",
    "    print(torch.softmax(output, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
